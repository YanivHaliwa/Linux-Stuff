#!/usr/bin/env python

import os
import sys
import argparse
import gc
from contextlib import contextmanager

# Suppress warnings and set environment variables before imports
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'       # Suppress TensorFlow logging
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['TRANSFORMERS_NO_TQDM'] = '1'       # Disable TQDM progress bars globally

import warnings
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', message='.*cuDNN.*')
warnings.filterwarnings('ignore', message='.*cuBLAS.*')
warnings.filterwarnings('ignore')               # Ignore all warnings

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('filelock').setLevel(logging.ERROR)

# Suppress absl logging
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)

# Suppress MKL warnings
try:
    from ctypes import cdll
    mkl = cdll.LoadLibrary('libmkl_rt.so')
    mkl.MKL_Set_Warning_Mode(2)
except:
    pass

# Context manager to suppress stderr during imports
@contextmanager
def suppress_stderr():
    stderr = sys.stderr
    stderr_fd = stderr.fileno()
    saved_stderr_fd = os.dup(stderr_fd)
    devnull = os.open(os.devnull, os.O_WRONLY)
    os.dup2(devnull, stderr_fd)
    try:
        yield
    finally:
        os.dup2(saved_stderr_fd, stderr_fd)
        os.close(saved_stderr_fd)
        os.close(devnull)

# Suppress stderr during imports that produce warnings
with suppress_stderr():
    import torch
    from torch.amp import autocast
    import whisper
    import openai
    import colorama
    from colorama import Fore, Style as ColoramaStyle
    from pydub import AudioSegment
    import numpy as np

# Import remaining modules
import faster_whisper
import tensorflow as tf
from transformers import logging as hf_logging
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Configure TensorFlow and Transformers logging
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel('ERROR')
hf_logging.set_verbosity(hf_logging.ERROR)




SAMPLING_RATE = 16000

def faster_model(audio_path,model_path,language="he"):
    try:
        print(f"Loading model from {model_path}...")
        model = faster_whisper.WhisperModel(model_path)
        print(f"Processing audio...")
        segs, _ = model.transcribe(audio_path,language)
        texts = [s.text for s in segs]
        transcribed_text = ' '.join(texts)
    except KeyboardInterrupt:
        print("\nexiting...\n")
        sys.exit(1)

    return transcribed_text.strip()
        
def transcribe_audio(audio_path, model_path, language="he"):
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"\n---------------------------------\nUsing device: {device}")

        print(f"Loading model from {model_path}...")
        model = WhisperForConditionalGeneration.from_pretrained(model_path)
        model = model.to(device)
        processor = WhisperProcessor.from_pretrained(model_path)


        if device.type == "cuda":
            model = model.half()
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False

        print(f"Loading and resampling audio...")
        audio = AudioSegment.from_file(audio_path)
        audio = audio.set_channels(1)  # Convert to mono
        audio = audio.set_frame_rate(SAMPLING_RATE)  # Resample to 16000 Hz

        # Convert audio to numpy array
        samples = audio.get_array_of_samples()
        audio_np = np.array(samples).astype(np.float32) / 32768.0  # Normalize to [-1.0, 1.0]

        # Process audio in chunks
        chunk_length_seconds = 30
        chunk_length_samples = chunk_length_seconds * SAMPLING_RATE
        chunks = [audio_np[i:i+chunk_length_samples] for i in range(0, len(audio_np), chunk_length_samples)]

        full_transcription = ""
        for i, chunk in enumerate(chunks):
            print(f"Processing chunk {i+1}/{len(chunks)}...\r", end="", flush=True)
            input_features = processor(chunk, sampling_rate=SAMPLING_RATE, return_tensors="pt").input_features
            input_features = input_features.to(device)

            # Calculate attention mask
            attention_mask = torch.ones_like(input_features)
            attention_mask = attention_mask.to(device)

            with torch.inference_mode():
                with torch.amp.autocast("cuda" if device.type == "cuda" else "cpu"):
                    predicted_ids = model.generate(
                        input_features,
                        attention_mask=attention_mask,
                        language=language,
                        num_beams=5,
                        forced_decoder_ids=None  # Remove forced_decoder_ids
                    )
                    chunk_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
                    full_transcription += chunk_transcription + " "

            torch.cuda.empty_cache()
    except KeyboardInterrupt:
        print("\nexiting...\n")
        sys.exit(1)

    return full_transcription.strip()


if __name__ == "__main__":

    class CustomHelpFormatter(argparse.RawTextHelpFormatter):
        def _split_lines(self, text, width):
            # Split lines manually to preserve new lines in help text
            if '\n' in text:
                return text.splitlines()
            return argparse.HelpFormatter._split_lines(self, text, width)

    parser = argparse.ArgumentParser(
        description="Transcribe audio using Ivrit-AI Whisper model",
        formatter_class=CustomHelpFormatter
    )

    parser.add_argument("audio_file", help="Path to the audio file")
        
    parser.add_argument("--model", default="ivrit-ai/whisper-v2-d3-e3", help=(
        "Model path to use for transcription [must use quotation marks]. Models:\n"
        "ivrit-ai/whisper-v2-d3-e3 (default)\n"
        "ivrit-ai/whisper-large-v2-tuned\n"
        "ivrit-ai/faster-whisper-v2-d3-e3\n"
        "ivrit-ai/faster-whisper-v2-d4\n"
        "openai/whisper-large-v3-turbo\n"
        "adarcook/whisper-large-v3-hebrew\n"
        "\n"
    ))

    parser.add_argument("--language", default="he", help="Language of the audio (default: he)")
    parser.add_argument("--output", help="Output text file path (default: same as input with .txt extension)")
    
    args = parser.parse_args()
    try:
        if "faster" in args.model:
            transcription=faster_model(args.audio_file, model_path=args.model, language=args.language)
        else:
            transcription = transcribe_audio(args.audio_file, model_path=args.model, language=args.language)
    except KeyboardInterrupt:
        print("\nexiting...\n")
        sys.exit(1)

    if args.output:
        output_file = args.output
    else:
        output_file = os.path.splitext(args.audio_file)[0] + ".txt"

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(transcription)

    print(f"\nTranscription saved to {output_file}")
    print("\nTranscription:")
    print(transcription)