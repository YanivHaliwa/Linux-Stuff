#!/usr/bin/env python

import torch
import whisper
from torch.cuda.amp import autocast
import gc
import os
import argparse
import sys
import openai
from openai import OpenAI
import colorama
from colorama import Fore, Style as ColoramaStyle
from pydub import AudioSegment
import numpy as np
import warnings

global alertOnce
alertOnce = False
colorama.init(autoreset=True)

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
modelSource = "gpt-4o"

os.environ["OMP_NUM_THREADS"] = "50"
os.environ["MKL_NUM_THREADS"] = "50"

def translate_gpt(uinput):
        try:          
            response = client.chat.completions.create(
            model=modelSource,
            messages=[
                        {
                            'role': 'system',
                            'content': f'''
                            you are going to improve subtitle file which done after transcribe from audio.
                            language is hebrew. 
                            be expert on fluent hebrew. even the slang.  
                            your respond should be only the fixing subs. nothing else.
                            i need you to think on the context.  
                            i also need you figure slang of people . dont just fix words but think how they use it and that sentence complete.
                            must remember you fixing srt files so i need you to be precise with timestamp and title numbers as in original exactly.
                            also remember you cant exaplain ANYTHING in process. you only ixing text accoding to source only.
                            do not use in your answer any qoute or other words that not in the original you stick to fix text only
                            do not add words "plaintext" in your text just if its exist in the original  
                            do not write anything but hebrew srt text ONLY        
                            '''
                        },
                       
                        {
                            "role": "user",
                            "content": uinput
                        },
                
            ],
            stream=True,
            temperature=1,
            max_tokens=2000
            )
            summary=""
            ch=""
            for chunk in response:
                ch = chunk.choices[0]
                txt=ch.delta.content
                if txt:
                    summary += txt             

        except openai.APIError as e:
            errors=str(e)
            error_message = errors.split("'type':")[0].split("'message':")[1].strip().strip(",").strip("'").strip()
            print(f"{error_message}")
            sys.exit(1)

        except openai.APIConnectionError as e:
            errors=str(e)
            error_message = errors.split("'type':")[0].split("'message':")[1].strip().strip(",").strip("'").strip()
            print(f"{error_message}")
            sys.exit(1)

        except openai.RateLimitError as e:
            errors=str(e)
            error_message = errors.split("'type':")[0].split("'message':")[1].strip().strip(",").strip("'").strip()
            print(f"{error_message}")
            sys.exit(1)

        except Exception as e:
            print(f"Unexpected error: {e}")
            sys.exit(1)

        except KeyboardInterrupt:
            print("\n")   
            inp = input(Fore.BLUE + ColoramaStyle.BRIGHT + 'You want to exit? (y/n): ' + ColoramaStyle.RESET_ALL).strip()
            if inp.lower() == "y":
                print("Exiting...")
                sys.exit(1)    
            else:
                print("Resuming...")
            
        return summary

def get_model_path(model_name):
    """Map model names to their file names"""
    model_mapping = {
        "turbo": "large-v3-turbo",
        "tiny": "tiny",
        "base": "base",
        "small": "small",
        "medium": "medium",
        "large": "large-v3",
    }
    name=model_mapping.get(model_name, model_name)
    return name


def load_whisper_model(model_name, device="cuda"):
    global alertOnce
    """Safely load whisper model with weights_only=True"""
    model_correct = get_model_path(model_name)
    user_model=model_name
    model_path = os.path.expanduser(f"~/.cache/whisper/{model_correct}") 
    fullpath_modelfile=os.path.join(model_path, model_correct) + ".pt"
    if alertOnce==False:
        print(f"User chose {ColoramaStyle.BRIGHT}{user_model}{ColoramaStyle.RESET_ALL} model which maps to {ColoramaStyle.BRIGHT}{model_correct}{ColoramaStyle.RESET_ALL} model file.")
        alertOnce=True

    # First check if model exists, download if needed
    if not os.path.exists(model_path):
        # download_name = "large-v3" if model_name == "turbo" else model_name
        download_name = model_correct
        whisper._download(whisper._MODELS[download_name], model_path, False)
    
    # Load the model weights safely with weights_only=True
    checkpoint = torch.load(fullpath_modelfile, map_location=device, weights_only=True)
    
    # Initialize model architecture
    dims = whisper.ModelDimensions(**checkpoint["dims"])
    model = whisper.Whisper(dims)
    
    # Load state dict
    model.load_state_dict(checkpoint["model_state_dict"])
    
    # Set precision based on device
    if device == "cuda":
        model = model.cuda()
        model = model.half()
    else:
        model = model.float()
        
    return model

def enable_checkpointing(model):
    if hasattr(model, 'encoder'):
        model.encoder.gradient_checkpointing = True
    if hasattr(model, 'decoder'):
        model.decoder.gradient_checkpointing = True
    return model

def format_timestamp(seconds):
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    seconds = seconds % 60
    return f"{int(hours):02d}:{int(minutes):02d}:{seconds:05.2f}"


def process_audio_chunk(chunk, model, device, language):
    if device == "cuda":
        with torch.amp.autocast(device_type='cuda'):  # Fixed deprecated autocast
            return model.transcribe(chunk, language=language, fp16=True)
    else:
        return model.transcribe(chunk, language=language, fp16=False)



def transcribe_audio(audio_path, model_name="large-v3", language=None):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    if device == "cuda":
        torch.cuda.empty_cache()
        gc.collect()

    try:
        print(f"Loading {model_name} model...")
        model = load_whisper_model(model_name, device)
        model = enable_checkpointing(model)

        print("Transcribing audio...")
        if device == "cuda":
            with torch.amp.autocast(device_type='cuda'):  # Fixed deprecated autocast
                result = model.transcribe(audio_path, language=language, fp16=True)
        else:
            result = model.transcribe(audio_path, language=language, fp16=False)

    except RuntimeError as e:
        if "out of memory" in str(e):
            print("GPU out of memory. Falling back to CPU...")
            torch.cuda.empty_cache()
            gc.collect()

            warnings.filterwarnings("ignore", category=UserWarning, module="whisper.transcribe")
            device = "cpu"
            model = load_whisper_model(model_name, device)
            model = enable_checkpointing(model)
            result = model.transcribe(audio_path, language=language, fp16=False)
        else:
            raise e

    finally:
        if device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()

    return result

def write_srt(result, output_file):
    with open(output_file, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(result["segments"], start=1):
            start = format_timestamp(segment["start"]).replace(".", ",")
            end = format_timestamp(segment["end"]).replace(".", ",")
            text = segment["text"].strip()
            f.write(f"{i}\n{start} --> {end}\n{text}\n\n")

def enhanceAI(srt_file):
    with open(srt_file, 'r', encoding='utf-8') as f:
        srt_content = f.read()
    enhanced_content = translate_gpt(srt_content)
    with open(srt_file, 'w', encoding='utf-8') as f:
        f.write(enhanced_content)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Transcribe audio using Whisper")
    parser.add_argument("audio_file", help="Path to the audio file")
    parser.add_argument("-m", "--model", default="turbo", help="Model to use for transcription: tiny,base,small,medium,large,turbo (default: turbo)")
    parser.add_argument("-l", "--lang", default=None, help="Language of the audio: en,he (default: auto-detect)")
    parser.add_argument("-o", "--output", help="Output SRT file path (default: same as input with .srt extension)")
    parser.add_argument("-e", "--enhance", action="store_true", help="Enhance SRT file with GPT after transcription")
    args = parser.parse_args()

    audio_path = args.audio_file
    # SAMPLING_RATE = 16000
    # print(f"Loading and resampling audio...")
    # audio = AudioSegment.from_file(audio_path)
    # audio = audio.set_channels(1)  # Convert to mono
    # audio = audio.set_frame_rate(SAMPLING_RATE)  # Resample to 16000 Hz

    # # Convert to numpy array and normalize
    # samples = np.array(audio.get_array_of_samples()).astype(np.float32)
    # if audio.sample_width == 2:  # Assuming 16-bit PCM
    #     samples /= 32768.0  # Normalize to [-1.0, 1.0]

    # # Make sure it's a 1D array and reshape if needed
    # audio_np = np.array(samples, dtype=np.float32)

    try:
        # Load and resample audio
        audio_path = args.audio_file
        print("Loading and resampling audio with Whisper defaults...")
        audio_np = whisper.load_audio(audio_path)  # Loads, resamples to 16kHz, converts to mono, and normalizes

        result = transcribe_audio(audio_np, model_name=args.model, language=args.lang)

        # Determine output file name
        if args.output:
            output_file = args.output
        else:
            output_file = os.path.splitext(args.audio_file)[0] + ".srt"
       
        # Write SRT file
        write_srt(result, output_file)

        # Enhance with Claude if -cl option is used
        if args.enhance:
            print("Enhancing SRT file with Claude...")
            enhanceAI(output_file)
            print(f"\nEnhanced transcription saved to {output_file}")

        print("\nTranscription with timestamps:")
        for segment in result["segments"]:
            start = format_timestamp(segment["start"])
            end = format_timestamp(segment["end"])
            text = segment["text"]
            print(f"[{start} --> {end}] {text}")
    except KeyboardInterrupt:
        print("\r\rexiting..")
        sys.exit(0)